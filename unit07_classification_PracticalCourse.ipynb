{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482fc931-9e01-407e-bfc1-ee74e632afc2",
   "metadata": {},
   "source": [
    "# Unit 07\n",
    "\n",
    "`````{tab-set}\n",
    "````{tab-item} Objective\n",
    "\n",
    "::::{important} Objective\n",
    "\n",
    "The objective of this hands-on session is to provide you with practical experience in applying supervised machine learning methods for classification tasks. Specifically, we will focus on k-nearest neighbor, logistic regression, and random forest classifier techniques.\n",
    "\n",
    "In this example, we will analyze a dataset containing class labels for 40 dyes, with one molecule missing a class label. Our goal is to train classification models that not only accurately describe the training and test data but also predict the class label for the one sample where it is missing.\n",
    "\n",
    "The reference data for training is obtained from [PhotochemCAD](https://www.photochemcad.com/).\n",
    "\n",
    "::::\n",
    "\n",
    "````\n",
    "\n",
    "\n",
    "````{tab-item} Further Information\n",
    "\n",
    ":::{admonition}Further Information\n",
    "\n",
    "\n",
    "**pandas** and **pandas.DataFrame**\n",
    "- [`pandas.DataFrames` (1)](https://pandas.pydata.org/docs/user_guide/10min.html)\n",
    "- [`pandas.DataFrames` (2)](https://www.w3schools.com/python/pandas/pandas_dataframes.asp)\n",
    "- [`pandas.DataFrames` (3)](https://www.datacamp.com/tutorial/pandas-tutorial-dataframe-python)\n",
    "\n",
    "**sklearn**\n",
    "- [k-Nearest Neighbours](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "- [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "- [`Pipelines`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "- [Decision Boundaries](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html)\n",
    "- [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "- [Pipelining and grid search](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html#pipelining)\n",
    "\n",
    ":::\n",
    "\n",
    "````\n",
    "`````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e325b775-a0ec-4dc6-a66c-085325499bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import digichem as dc\n",
    "from digichem import FP, Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fdc75f3-78ad-49c2-9a7f-1fc5521f68db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dye_category</th>\n",
       "      <th>CAS</th>\n",
       "      <th>name</th>\n",
       "      <th>smiles</th>\n",
       "      <th>abs_max</th>\n",
       "      <th>solvent</th>\n",
       "      <th>charge</th>\n",
       "      <th>counter_ion</th>\n",
       "      <th>RDmol</th>\n",
       "      <th>MorganFP_r3_s512</th>\n",
       "      <th>MorganFP_r3_s1024</th>\n",
       "      <th>MorganFP_r4_s512</th>\n",
       "      <th>MorganFP_r4_s1024</th>\n",
       "      <th>abs_spec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>K01</th>\n",
       "      <td>cyanines</td>\n",
       "      <td>977-96-8</td>\n",
       "      <td>1,1'-Diethyl-2,2'-cyanine iodide</td>\n",
       "      <td>CCN1/C(=C/c2ccc3ccccc3[n+]2CC)C=Cc2ccccc21</td>\n",
       "      <td>524.25</td>\n",
       "      <td>ethanol</td>\n",
       "      <td>1</td>\n",
       "      <td>[I-]</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f264f9cdb70&gt;</td>\n",
       "      <td>512 bit FP</td>\n",
       "      <td>1024 bit FP</td>\n",
       "      <td>512 bit FP</td>\n",
       "      <td>1024 bit FP</td>\n",
       "      <td>220.0 to 900.0 nm, steps: 1.0 nm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K02</th>\n",
       "      <td>cyanines</td>\n",
       "      <td>605-91-4</td>\n",
       "      <td>1,1'-Diethyl-2,2'-carbocyanine iodide</td>\n",
       "      <td>CCN1/C(=C/C=C/c2ccc3ccccc3[n+]2CC)C=Cc2ccccc21</td>\n",
       "      <td>603.50</td>\n",
       "      <td>methanol</td>\n",
       "      <td>1</td>\n",
       "      <td>[I-]</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f264f9cdc60&gt;</td>\n",
       "      <td>512 bit FP</td>\n",
       "      <td>1024 bit FP</td>\n",
       "      <td>512 bit FP</td>\n",
       "      <td>1024 bit FP</td>\n",
       "      <td>220.0 to 900.0 nm, steps: 1.0 nm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K03</th>\n",
       "      <td>cyanines</td>\n",
       "      <td>14187-31-6</td>\n",
       "      <td>1,1'-Diethyl-2,2'-dicarbocyanine iodide</td>\n",
       "      <td>CCN1/C(=C/C=C/C=C/c2ccc3ccccc3[n+]2CC)C=Cc2ccc...</td>\n",
       "      <td>711.00</td>\n",
       "      <td>ethanol</td>\n",
       "      <td>1</td>\n",
       "      <td>[I-]</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f264f9cdd00&gt;</td>\n",
       "      <td>512 bit FP</td>\n",
       "      <td>1024 bit FP</td>\n",
       "      <td>512 bit FP</td>\n",
       "      <td>1024 bit FP</td>\n",
       "      <td>220.0 to 900.0 nm, steps: 1.0 nm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K04</th>\n",
       "      <td>cyanines</td>\n",
       "      <td>4727-49-5</td>\n",
       "      <td>1,1'-Diethyl-4,4'-cyanine iodide</td>\n",
       "      <td>CCN1C=C/C(=C\\c2cc[n+](CC)c3ccccc23)c2ccccc21</td>\n",
       "      <td>592.00</td>\n",
       "      <td>ethanol</td>\n",
       "      <td>1</td>\n",
       "      <td>[I-]</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f264f9cdda0&gt;</td>\n",
       "      <td>512 bit FP</td>\n",
       "      <td>1024 bit FP</td>\n",
       "      <td>512 bit FP</td>\n",
       "      <td>1024 bit FP</td>\n",
       "      <td>220.0 to 900.0 nm, steps: 1.0 nm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K05</th>\n",
       "      <td>cyanines</td>\n",
       "      <td>4727-50-8</td>\n",
       "      <td>1,1'-Diethyl-4,4'-carbocyanine iodide</td>\n",
       "      <td>CCN1C=C/C(=C\\C=C\\c2cc[n+](CC)c3ccccc23)c2ccccc21</td>\n",
       "      <td>709.50</td>\n",
       "      <td>ethanol</td>\n",
       "      <td>1</td>\n",
       "      <td>[I-]</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f264f9cde40&gt;</td>\n",
       "      <td>512 bit FP</td>\n",
       "      <td>1024 bit FP</td>\n",
       "      <td>512 bit FP</td>\n",
       "      <td>1024 bit FP</td>\n",
       "      <td>220.0 to 900.0 nm, steps: 1.0 nm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dye_category         CAS                                     name  \\\n",
       "K01     cyanines    977-96-8         1,1'-Diethyl-2,2'-cyanine iodide   \n",
       "K02     cyanines    605-91-4    1,1'-Diethyl-2,2'-carbocyanine iodide   \n",
       "K03     cyanines  14187-31-6  1,1'-Diethyl-2,2'-dicarbocyanine iodide   \n",
       "K04     cyanines   4727-49-5         1,1'-Diethyl-4,4'-cyanine iodide   \n",
       "K05     cyanines   4727-50-8    1,1'-Diethyl-4,4'-carbocyanine iodide   \n",
       "\n",
       "                                                smiles  abs_max   solvent  \\\n",
       "K01         CCN1/C(=C/c2ccc3ccccc3[n+]2CC)C=Cc2ccccc21   524.25   ethanol   \n",
       "K02     CCN1/C(=C/C=C/c2ccc3ccccc3[n+]2CC)C=Cc2ccccc21   603.50  methanol   \n",
       "K03  CCN1/C(=C/C=C/C=C/c2ccc3ccccc3[n+]2CC)C=Cc2ccc...   711.00   ethanol   \n",
       "K04       CCN1C=C/C(=C\\c2cc[n+](CC)c3ccccc23)c2ccccc21   592.00   ethanol   \n",
       "K05   CCN1C=C/C(=C\\C=C\\c2cc[n+](CC)c3ccccc23)c2ccccc21   709.50   ethanol   \n",
       "\n",
       "     charge counter_ion                                             RDmol  \\\n",
       "K01       1        [I-]  <rdkit.Chem.rdchem.Mol object at 0x7f264f9cdb70>   \n",
       "K02       1        [I-]  <rdkit.Chem.rdchem.Mol object at 0x7f264f9cdc60>   \n",
       "K03       1        [I-]  <rdkit.Chem.rdchem.Mol object at 0x7f264f9cdd00>   \n",
       "K04       1        [I-]  <rdkit.Chem.rdchem.Mol object at 0x7f264f9cdda0>   \n",
       "K05       1        [I-]  <rdkit.Chem.rdchem.Mol object at 0x7f264f9cde40>   \n",
       "\n",
       "    MorganFP_r3_s512 MorganFP_r3_s1024 MorganFP_r4_s512 MorganFP_r4_s1024  \\\n",
       "K01       512 bit FP       1024 bit FP       512 bit FP       1024 bit FP   \n",
       "K02       512 bit FP       1024 bit FP       512 bit FP       1024 bit FP   \n",
       "K03       512 bit FP       1024 bit FP       512 bit FP       1024 bit FP   \n",
       "K04       512 bit FP       1024 bit FP       512 bit FP       1024 bit FP   \n",
       "K05       512 bit FP       1024 bit FP       512 bit FP       1024 bit FP   \n",
       "\n",
       "                             abs_spec  \n",
       "K01  220.0 to 900.0 nm, steps: 1.0 nm  \n",
       "K02  220.0 to 900.0 nm, steps: 1.0 nm  \n",
       "K03  220.0 to 900.0 nm, steps: 1.0 nm  \n",
       "K04  220.0 to 900.0 nm, steps: 1.0 nm  \n",
       "K05  220.0 to 900.0 nm, steps: 1.0 nm  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "with open('db_dyes_07.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c9d36-9f4f-4a48-9476-bbffbca3b0ab",
   "metadata": {},
   "source": [
    "## A) Inspection and pre-processing of data\n",
    "\n",
    "::::{tip} Task A1\n",
    "\n",
    "Although training classifiers on multidimensional features (vectors, arrays, tensors) and multiple features is not problematic, the goal of this hands-on session is to train classifiers using only two features. \n",
    "These two features will be scalar values for each molecule, allowing us to easily visualize the decision boundaries and visually inspect the model's quality.\n",
    "\n",
    "To achieve this, we will derive two features from the absorption spectrum of each dye. \n",
    "\n",
    "STEP 1 - Plotting Absorption Spectra\n",
    ": Plot the absorption spectra of all dyes labeled 'acridines' and those labeled 'cyanines'. \n",
    "\n",
    "STEP 2 - Feature Design\n",
    ": Analyze these spectra to identify common and distinguishing features. Based on your observations, devise a strategy to extract two features from the absorption spectra and store them in the DataFrame.\n",
    "\n",
    "STEP 3 - Visualize Features\n",
    ": Plot the distribution of the data points based on the two selected features. Assess if the features are suitable for classification tasks. (Note: Be sure to color-highlight the data points according to their respective class labels.)\n",
    "\n",
    "\n",
    ":::{admonition} Variables and Features in the Database\n",
    ":class: dropdown\n",
    "CLASS OF DYE ('dye_category')\n",
    ":  classification of the objects into acridines or cyanines\n",
    "\n",
    "CAS ('CAS')\n",
    ": CAS identifier of the objects\n",
    "\n",
    "NAME ('name')\n",
    ":  IUPAC name of the objects\n",
    "\n",
    "SMILES ('smiles')\n",
    ": SMILES string of each object\n",
    "\n",
    "VISIBLE ABSORPTION MAXIMUM ('abs_max')\n",
    ": localization of the experimental absorption maximum of a dye in the visible region (360-800 nm) in the solvent indicated in column 'solvent'\n",
    "\n",
    "CHARGE ('charge')\n",
    ": total charge of the molecules, the respective counter-ions are given in column 'counter_ion' \n",
    "\n",
    "Morgan Fingerprints (e.g. 'MorganFP_r3_s512')\n",
    ": Morgan fingerprints from unit 06 that were generated using a radius of 3 or 4 (indicated by the 'r') and a final fingerprint-size of either 512 or 1024 bits (indicated by 's')\n",
    "\n",
    "Principal Components (e.g. 'PC1_r3_s512')\n",
    ": Two principal components (PC1 and PC2) from PCA analysis of the Morgan fingerprints, which were generated using a radius of 3 or 4 (indicated by the 'r') and a final fingerprint-size of either 512 or 1024 bits (indicated by 's')\n",
    "\n",
    "Absorption Spectrum ('abs_spec')\n",
    ": Experimental UVvis absorption spectrum of the dyes. The array consisting of the absorption wavelengths in the first column and the mean molar extinction coefficients in the second column, can be accesed by e.g. ```[x.spec for x in df['abs_spec']]```\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    ":::{admonition} Data Overview from pd.DataFrame\n",
    ":class: dropdown\n",
    "\n",
    "An overview of the number of objects and variables is returned by\n",
    "\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 1\n",
    ":emphasize-lines: 1\n",
    "\n",
    "df_dyes.count()\n",
    "```\n",
    "\n",
    "When interested in the counts of a certain dye class, one can for instance count values of a certain column. \n",
    "The values of a column can be accessed via:\n",
    "\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 2\n",
    ":emphasize-lines: 2\n",
    "\n",
    "df_dyes['dye_category'].values\n",
    "```\n",
    "\n",
    "Generally, one can just call the function `hist()` to plot histogramms of all numeric columns from a dataframe. \n",
    "\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 3\n",
    ":emphasize-lines: 3\n",
    "\n",
    "df_dyes.hist()\n",
    "```\n",
    ":::\n",
    "\n",
    "\n",
    ":::{admonition} Access spectra based on respective labels\n",
    ":class: dropdown\n",
    "\n",
    "Access the spectra from the DataFrame based on the respective labels to facilitate your analysis.\n",
    "You can use the following code snippet to get rid of the absorption spectrum of the unlabelled data point (molecule with the `unknown` class.) and access only the absorption data of the cyanines and acridines.\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 4\n",
    ":emphasize-lines: 4\n",
    "\n",
    "spec_acridines_cyanines = df[df['dye_category'] != 'unknown']['abs_spec']\n",
    "```\n",
    "\n",
    "To get the array of wavelengths and respective extinction coefficients from the objects stored in the dataframe, you can do the following.\n",
    "Generally, the array of the spectrum is accessed by the `spec` function:\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 5\n",
    ":emphasize-lines: 5\n",
    "\n",
    "arr_acridines_cyanines = [(idx, spec_acridines_cyanines.loc[idx].spec) for idx in spec_acridines_cyanines.index]\n",
    "```\n",
    ":::\n",
    "\n",
    ":::{admonition} Lineplots with Matplotlib\n",
    ":class: dropdown\n",
    "\n",
    "Documentation: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 6\n",
    ":emphasize-lines: 9\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for spectrum in arr_acridines_cyanines:\n",
    "    plt.plot(spectrum[:, 0], spectrum[:, 1])\n",
    "```\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ada108-b8ec-4bf2-83a0-c040909f361f",
   "metadata": {},
   "source": [
    "### Step 1 - Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e05ec327-1e6a-463f-9bd1-a5008cbb9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spectra of cyanines and acridines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0c0b48-19e3-4f89-8b89-a4867b0cd6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot spectra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7a69c9-da8e-43b0-8974-1d7f0e43edf3",
   "metadata": {},
   "source": [
    "### Step 2 - Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14cdd11c-b500-420a-a746-e36029e002ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction: calculate the two features and dump them to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5ef25-194b-4adb-8b04-fe5747fd71e7",
   "metadata": {},
   "source": [
    "### Step 3 - Visualization of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64df1723-5444-4879-b099-2a2f8fb8f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot of features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58327ab9-7072-4df3-9919-ea145b2ccdb5",
   "metadata": {},
   "source": [
    "::::{tip} Task A2 \n",
    "\n",
    "Prior to the supervised machine learning for classification, we have to split our data into a set of training and test data and furthermore prepare the unlabelled data point:\n",
    "\n",
    "Please create a ```pd.DataFrame``` of the labeled data, including two columns for the features derived from the absorption spectra of the dyes.\n",
    "Additionally, create a ```pd.Series``` for the respective labels (`acridines` or `cyanines`).\n",
    "\n",
    "Ensure that the unlabeled data point is separated from the original dataset and is not included in either the training or test sets. \n",
    "Prepare this unlabeled data point in a similar pd.DataFrame format, as it will be used to predict the label using our trained model.\n",
    "\n",
    ":::{admonition} Data Splitting (sklearn)\n",
    ":class: dropdown\n",
    "\n",
    "Before performing supervised machine learning for classification, it is crucial to properly split your data into training and test sets to evaluate the model's performance accurately. This ensures that the model generalizes well to new, unseen data. Improper data splitting can lead to misleading performance metrics and overfitting.\n",
    "\n",
    "Typically, 70-80% of the data is used for training, and the remaining 20-30% is used for testing. This balance allows the model to learn effectively while having enough data to validate its performance.\n",
    "\n",
    "To split your data in Python using scikit-learn, you can use the train_test_split function. Here is an example:\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 1\n",
    ":emphasize-lines: 4\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'X' is your DataFrame of features and 'y' is your Series of labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 'test_size=0.2' means 20% of the data will be used for testing\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de3c4b32-9a13-4777-b693-75d6bc843ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data splitting\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ac82e-06a5-458b-b747-7c0294ab4682",
   "metadata": {},
   "source": [
    "## B) Supervised Classification\n",
    "\n",
    "The objective in section B is to train different classifiers to distinguish acridines and cyanines based on the two features extracted from the absorption data of the respective dyes.\n",
    "\n",
    ":::::{tip} Task B1 - kNN\n",
    "\n",
    "In a first step, we want to perform k-nearest neighbor classification.\n",
    "\n",
    "::::{admonition} Step 1: Initialize a kNN model\n",
    ":class: dropdown\n",
    "\n",
    "The standard workflow in working with sklearn models, can be described as follows using the example of kNN:\n",
    "\n",
    "STEP 1 - Import Libraries \n",
    ": We start by importing necessary classes from scikit-learn. These include KNeighborsClassifier for the kNN algorithm, Pipeline to streamline the preprocessing and modeling steps, and StandardScaler to standardize the features.\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 1\n",
    ":emphasize-lines: 2,3\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "\n",
    "STEP 2 - Define the Scaler\n",
    ": We create an instance of StandardScaler which standardizes features by removing the mean and scaling to unit variance. This is an essential preprocessing step to ensure the kNN algorithm performs well, as it relies on distance calculations.\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 4\n",
    ":emphasize-lines: 5\n",
    "\n",
    "# define scaling (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "```\n",
    "\n",
    "STEP 3 - Define the kNN Model\n",
    ": We instantiate the KNeighborsClassifier with n_neighbors=2, which means the algorithm will consider the 2 nearest neighbors to make a classification decision.\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 6\n",
    ":emphasize-lines: 7\n",
    "\n",
    "# kNN considering 2 nearest neighhbors for classification\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "```\n",
    "\n",
    "STEP 4 - Create a Pipeline\n",
    ": We combine the scaler and kNN model into a pipeline. This ensures that the data is first scaled before being passed to the kNN classifier. The pipeline helps in maintaining a clean and efficient workflow.\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 8\n",
    ":emphasize-lines: 9\n",
    "\n",
    "# pipeline combining scaling and kNN model \n",
    "knn_clf = Pipeline(steps=[(\"scaler\", scaler), (\"knn\", knn)])\n",
    "```\n",
    "\n",
    "STEP 5 - Fit the Model\n",
    ": We fit the pipeline to the training data (X_train and y_train). In this context, fitting means the scaler will compute the mean and variance from the training data, which will be used to transform the data. The kNN model will then use this transformed data for making predictions.\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 10\n",
    ":emphasize-lines: 11\n",
    "\n",
    "# train model (remember: kNN has no training but is instance-based/lazy learner)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    ":::{admonition} Complete Code\n",
    ":class: dropdown\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 1\n",
    ":emphasize-lines: 12\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# define scaling (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# kNN considering 2 nearest neighhbors for classification\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "# pipeline combining scaling and kNN model \n",
    "knn_clf = Pipeline(steps=[(\"scaler\", scaler), (\"knn\", knn)])\n",
    "\n",
    "# train model (remember: kNN has no training but is instance-based/lazy learner)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "```\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "::::{admonition} Step 2: Finding the Optimal k-Value for kNN Classification\n",
    ":class: dropdown\n",
    "\n",
    "Upon fitting the kNN model with a specific k-value, you can access the score, which indicates how accurately the model classifies the labeled data. For example, a score of 1.0 on the test set means the model correctly classified every test data point.\n",
    "\n",
    "In python you can access the scores and predictions with the score and predcit functions:\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 12\n",
    "\n",
    "# score (function takes features and labels and returns score)\n",
    "knn_clf.score(X_test, y_test)\n",
    "\n",
    "# prediction (function takes only features and returns a label)\n",
    "knn_clf.score(X_sample)\n",
    "```\n",
    "\n",
    "To find the optimal k-value, perform kNN classification for various k-values and evaluate their scores. By comparing these scores, you can determine the best k-value for your model.\n",
    "Instructions:\n",
    "\n",
    "1. Perform kNN classification for a range of k-values.\n",
    "2. Calculate the scores for both the training and test data for each k-value.\n",
    "3. Visualize how the scores on the test and training data depend on the k-value.\n",
    "3. Based on the visualization, choose the optimal k-value.\n",
    "\n",
    "::::\n",
    "\n",
    "::::{admonition} Step 3: Assessing the Model's Quality\n",
    ":class: dropdown\n",
    "\n",
    "Now that we have chosen the optimal k-value, we will train a kNN model using this value and analyze its performance. To do this, we will examine two different measures: the confusion matrix and decision boundaries.\n",
    "\n",
    "1. Train the kNN model using the optimal k-value.\n",
    "2. Generate and analyze the confusion matrix for the test data.\n",
    "3. Visualize the decision boundaries of the trained model. Visualize the data point with unknown label. Is the model able to describe the class of this dye correct?\n",
    "\n",
    ":::{admonition} Confusion Matrix\n",
    ":class: dropdown\n",
    "\n",
    "The confusion matrix is a table that summarizes the performance of a classification algorithm. It shows the number of correct and incorrect predictions made by the model compared to the actual classifications. Each row of the matrix represents the instances in an actual class, while each column represents the instances in a predicted class. This matrix helps to assess the model's quality by providing insights into its accuracy, precision, recall, and ability to distinguish between different classes.\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 1\n",
    ":emphasize-lines: 6\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# create confusion matrix\n",
    "cm = confusion_matrix(y_test, knn_clf.predict(X_test))\n",
    "\n",
    "# convert array into pd.DataFrame\n",
    "cm_matrix = pd.DataFrame(\n",
    "    data=cm, \n",
    "    columns=['Actual: Acridine', 'Actual: Cyanine'], \n",
    "    index=['Predict: Acridine', 'Predict: Cyanine']\n",
    ")\n",
    "\n",
    "# visualize the confusion matrix as heatmap\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='rocket_r')\n",
    "```\n",
    ":::\n",
    "\n",
    ":::{admonition} Decision Boundaries\n",
    ":class: dropdown\n",
    "\n",
    "Decision boundaries are the surfaces that separate different classes predicted by the model. In the context of kNN, these boundaries illustrate how the algorithm divides the feature space based on the training data. Visualizing decision boundaries helps to assess the model's quality by showing how well the model generalizes to new data and how it handles different regions of the feature space. It can reveal whether the model is overfitting or underfitting the data.\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 1\n",
    ":emphasize-lines: 4,5,6,7,8,9,10\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# compute and visualize descion boundaries from the classifier\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    knn_clf, \n",
    "    X_all.values, \n",
    "    response_method=\"predict\",\n",
    "    xlabel='feature 1', \n",
    "    ylabel='feature 2',\n",
    ")\n",
    "\n",
    "# indicate class by color in scatterplot\n",
    "color_mapping = {'cyanines': 'blue', 'acridines': 'orange'}\n",
    "colors = [color_mapping[label] for label in y_all]\n",
    "\n",
    "disp.ax_.scatter(X_all['feature 1'].values, X_all['feature 2'].values, c=colors)\n",
    "```\n",
    ":::\n",
    "::::\n",
    "\n",
    ":::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f5307b3-8904-4595-8c6b-a50e6a81a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d16cd912-c5af-4ce4-a1dc-8ea31c27d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial kNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86327daf-a6d0-413d-810c-91b1e5b2bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning of k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b1c294-363c-4933-b4aa-24ff3f7e832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using optimal k-value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d09bad2a-c15d-4b58-9644-60e68ad785da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "269d3597-6f5b-433c-9391-eed6ed51ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision boundaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00180de1-c0bd-4009-8738-2c489d8e091d",
   "metadata": {},
   "source": [
    ":::::{tip} Task B2 - Logistic Regression\n",
    "\n",
    "Now, we want to perform Logistic Regression classification.\n",
    "\n",
    "::::{admonition} Step 1: Logistic Regression Pipeline\n",
    ":class: dropdown\n",
    "\n",
    "Set up a pipeline for a logistic regression model and fit the model with an initial set of parameters (e.g., default parameters).\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 1\n",
    ":emphasize-lines: 1\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "```\n",
    "::::\n",
    "\n",
    "::::{admonition} Step 2: Hyperparameter Tuning\n",
    ":class: dropdown\n",
    "\n",
    "After fitting the logistic regression model, we can tune the parameters, a process known as hyperparameter tuning. \n",
    "To find the optimal set of parameters, perform a grid search, testing different combinations of parameters and choosing the combination that gives the best score.\n",
    "\n",
    "In the following example, we assume we have initialized a scaling function and a logistic regression model called scaler and logistic.\n",
    "Next, define the grid of parameters you want to investigate, specifically a set of values for `C` (Inverse of regularization strength; smaller values specify stronger regularization).\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 2\n",
    ":emphasize-lines: 16\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scaler = StandardScaler()\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "\n",
    "logr_clf = Pipeline(\n",
    "    steps=[(\"scaler\", scaler), (\"logr\", logistic)]\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'logr__C': np.logspace(-4, 4, 20)\n",
    "}\n",
    "\n",
    "search = GridSearchCV(logr_clf, param_grid, n_jobs=2)\n",
    "search.fit(X_train,y_train)\n",
    "```\n",
    "\n",
    "1. Learn about the parameters of a logistic regression [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "2. Add an additional property to the `param_grid` dictionary and perform the grid search using this new grid. Which combination of parameters gives the best results?\n",
    "\n",
    "::::\n",
    "\n",
    "::::{admonition} Step 3: Assessing the Model's Quality\n",
    ":class: dropdown\n",
    "\n",
    "Now that we have performed the grid search to identify the best performing hyperparameters, train a logistic regression model using these parameters and assess the model quality based on the confusion matrix and decision boundaries.\n",
    "\n",
    "1. Train a logistic regression classifier using the best set of parameters from Step 2.\n",
    "2. Generate and analyze the confusion matrix for the test data.\n",
    "3. Visualize the decision boundaries of the trained model. Include the data point with the unknown label. Does the model correctly classify this dye?\n",
    "\n",
    "::::\n",
    ":::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74cf5b20-2135-4ece-8e94-8f46db7b095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initial model\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a91b23-13f7-4a50-a7f0-964c39a0131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a850a680-2508-4195-9361-159335c43c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Train model with best parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd2dd63a-d2b4-47bc-9e28-0d0010c2cb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Decision boundaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e2173f-610a-4a50-ac6f-b66fcedb5137",
   "metadata": {},
   "source": [
    ":::::{tip} Task B3 - Random Forest Classification\n",
    "\n",
    "Now, we want to perform a Random Forest classification.\n",
    "\n",
    "1. Setup the Pipeline for a Random Forest classification\n",
    "2. Tune the follwoing hyperparameters: max_depth, min_samples_split, criterion\n",
    "3. Visualize the Confusion matrix and decision boundaries of the model with the best parameters.\n",
    "\n",
    "::::{admonition} Random Forest in sklearn\n",
    "\n",
    "```{code-block} python\n",
    ":lineno-start: 1\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "```\n",
    "::::\n",
    "\n",
    ":::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f9e0aac-703f-47c6-9375-d20baea191bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8ed2e1c-54f9-4634-8a99-4f79bd91a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d6d40b0-1f33-4a9a-bda2-f072b18347d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3\n",
    "# model with tuned hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7cddea-0dac-4b64-a3e4-f8b0cf91ea04",
   "metadata": {},
   "source": [
    ":::::{tip} Task B4 - Choose a model for the task/dataset \n",
    "\n",
    "Now that we have configured three classifiers with optimized hyperparameters—namely k-Nearest Neighbors, Logistic Regression, and Random Forest Classifier—let's use all three models to predict the class of the unlabelled data point. Examine the details of the unlabelled molecule and determine if the models accurately classify its class.\n",
    "\n",
    "Evaluate which model is best suited for performing classification based on the chosen two features. Then, repeat the classification task using the entire absorption spectrum of each dye as features. Analyze if this approach improves the model's performance.\n",
    "\n",
    ":::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82ce8a31-1cc3-4286-80f1-ef1fc5506cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model predictions for unlabelled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcab9073-e91c-4e1e-8d3f-16113e58dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize unlabelled mol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93824513-9058-4064-931d-3d7b76546b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat classification of your choice with whole absorption spectrum as feature vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c791a6-85bc-4c2f-a52c-695c9a5b6a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
